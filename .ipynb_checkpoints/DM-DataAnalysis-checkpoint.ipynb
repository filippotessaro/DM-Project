{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/groceries.csv\n",
      "./data/test.json\n",
      "./data/train.json\n",
      "./data/groceries - groceries.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.special import comb\n",
    "from itertools import combinations, permutations\n",
    "\n",
    "import os\n",
    "\n",
    "# List all datests files\n",
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/groceries.csv', delimiter='\\n', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rolls/buns'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = pd.read_json ('./data/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6714\n"
     ]
    }
   ],
   "source": [
    "sum([len(x) for x in json_df.ingredients])\n",
    "\n",
    "list_ingredients = set()\n",
    "for item in json_df.ingredients:\n",
    "    #item = [lower(i) for i in item]\n",
    "    list_ingredients.update(item)\n",
    "    \n",
    "print(len(list_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]\n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique fields\n",
    "#print(json_df['cuisine'].unique())\n",
    "#print(json_df['ingredients'].unique())\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "def itemParser(row):\n",
    "    word_ps = []\n",
    "    for s in row:\n",
    "        \n",
    "        #REGULAR EXPRESSIONS:\n",
    "\n",
    "        #Remove punctuations\n",
    "        s=re.sub(r'[^\\w\\s]','',s)\n",
    "\n",
    "        #Remove Digits\n",
    "        s=re.sub(r\"(\\d)\", \"\", s)\n",
    "\n",
    "        #Remove content inside paranthesis\n",
    "        s=re.sub(r'\\([^)]*\\)', '', s)\n",
    "\n",
    "        #Remove Brand Name\n",
    "        s=re.sub(u'\\w*\\u2122', '', s)\n",
    "\n",
    "        #Convert to lowercase\n",
    "        s=s.lower()\n",
    "\n",
    "        #print(ps.stem(s))\n",
    "        #Remove Stop Words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = word_tokenize(s)\n",
    "        \n",
    "        filtered_sentence = [ps.stem(w) for w in word_tokens if not w in stop_words]\n",
    "        #print(filtered_sentence)\n",
    "        s=' '.join(filtered_sentence)\n",
    "        #print(\"join:\" + s)\n",
    "        word_ps.append(s)\n",
    "    return word_ps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['romaine lettuce',\n",
       " 'black olives',\n",
       " 'grape tomatoes',\n",
       " 'garlic',\n",
       " 'pepper',\n",
       " 'purple onion',\n",
       " 'seasoning',\n",
       " 'garbanzo beans',\n",
       " 'feta cheese crumbles']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.ingredients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pepper bacon', 'test']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemParser([\"peppers and bacon\", \"test\"])#json_df.ingredients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cuisine     id                                        ingredients  \\\n",
      "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...   \n",
      "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...   \n",
      "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...   \n",
      "3       indian  22213                [water, vegetable oil, wheat, salt]   \n",
      "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe...   \n",
      "\n",
      "                                     new_ingredients  \n",
      "0  [romain lettuc, black oliv, grape tomato, garl...  \n",
      "1  [plain flour, ground pepper, salt, tomato, gro...  \n",
      "2  [egg, pepper, salt, mayonais, cook oil, green ...  \n",
      "3                    [water, veget oil, wheat, salt]  \n",
      "4  [black pepper, shallot, cornflour, cayenn pepp...  \n"
     ]
    }
   ],
   "source": [
    "tmp = json_df[0:5].copy()\n",
    "print(tmp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_df['new_ingredients'] = json_df.apply(lambda x: itemParser(x.ingredients), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [romain lettuc, black oliv, grape tomato, garl...\n",
       "1    [plain flour, ground pepper, salt, tomato, gro...\n",
       "2    [egg, pepper, salt, mayonais, cook oil, green ...\n",
       "3                      [water, veget oil, wheat, salt]\n",
       "4    [black pepper, shallot, cornflour, cayenn pepp...\n",
       "Name: new_ingredients, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['new_ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df[:10]\n",
    "json_df.to_csv('./data/train-clean.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['romaine lettuce,black olives,grape tomatoes,garlic,pepper,purple onion,seasoning,garbanzo beans,feta cheese crumbles', 'plain flour,ground pepper,salt,tomatoes,ground black pepper,thyme,eggs,green tomatoes,yellow corn meal,milk,vegetable oil', 'eggs,pepper,salt,mayonaise,cooking oil,green chilies,grilled chicken breasts,garlic powder,yellow onion,soy sauce,butter,chicken livers']\n",
      "       cuisine     id                                        ingredients  \\\n",
      "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...   \n",
      "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...   \n",
      "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...   \n",
      "3       indian  22213                [water, vegetable oil, wheat, salt]   \n",
      "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe...   \n",
      "5     jamaican   6602  [plain flour, sugar, butter, eggs, fresh ginge...   \n",
      "6      spanish  42779  [olive oil, salt, medium shrimp, pepper, garli...   \n",
      "7      italian   3735  [sugar, pistachio nuts, white almond bark, flo...   \n",
      "8      mexican  16903  [olive oil, purple onion, fresh pineapple, por...   \n",
      "9      italian  12734  [chopped tomatoes, fresh basil, garlic, extra-...   \n",
      "\n",
      "                                                 ing  \\\n",
      "0  romaine lettuce,black olives,grape tomatoes,ga...   \n",
      "1  plain flour,ground pepper,salt,tomatoes,ground...   \n",
      "2  eggs,pepper,salt,mayonaise,cooking oil,green c...   \n",
      "3                     water,vegetable oil,wheat,salt   \n",
      "4  black pepper,shallots,cornflour,cayenne pepper...   \n",
      "5  plain flour,sugar,butter,eggs,fresh ginger roo...   \n",
      "6  olive oil,salt,medium shrimp,pepper,garlic,cho...   \n",
      "7  sugar,pistachio nuts,white almond bark,flour,v...   \n",
      "8  olive oil,purple onion,fresh pineapple,pork,po...   \n",
      "9  chopped tomatoes,fresh basil,garlic,extra-virg...   \n",
      "\n",
      "                                             ing_mod  \n",
      "0  romain lettuc , black oliv , grape tomato , ga...  \n",
      "1  plain flour , ground pepper , salt , tomato , ...  \n",
      "2  egg , pepper , salt , mayonais , cook oil , gr...  \n",
      "3                   water , veget oil , wheat , salt  \n",
      "4  black pepper , shallot , cornflour , cayenn pe...  \n",
      "5  plain flour , sugar , butter , egg , fresh gin...  \n",
      "6  oliv oil , salt , medium shrimp , pepper , gar...  \n",
      "7  sugar , pistachio nut , white almond bark , fl...  \n",
      "8  oliv oil , purpl onion , fresh pineappl , pork...  \n",
      "9  chop tomato , fresh basil , garlic , extra-vir...  \n"
     ]
    }
   ],
   "source": [
    "'''# Unique fields\n",
    "#print(json_df['cuisine'].unique())\n",
    "#print(json_df['ingredients'].unique())\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('english')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "new = []\n",
    "for s in json_df['ingredients']:\n",
    "    #print(str(s) + \"\\n\")\n",
    "    s = ','.join(s)\n",
    "    new.append(s)\n",
    "    \n",
    "print(new[0:3])\n",
    "\n",
    "json_df['ing'] = new\n",
    "\n",
    "ingredients = list()\n",
    "\n",
    "l=[]\n",
    "for s in json_df['ing']:\n",
    "    #REGULAR EXPRESSIONS:\n",
    "    \n",
    "    #Remove punctuations\n",
    "    #s=re.sub(r'[^\\w\\s]','',s)\n",
    "    \n",
    "    #Remove Digits\n",
    "    s=re.sub(r\"(\\d)\", \"\", s)\n",
    "    \n",
    "    #Remove content inside paranthesis\n",
    "    s=re.sub(r'\\([^)]*\\)', '', s)\n",
    "    \n",
    "    #Remove Brand Name\n",
    "    s=re.sub(u'\\w*\\u2122', '', s)\n",
    "    \n",
    "    #Convert to lowercase\n",
    "    s=s.lower()\n",
    "    \n",
    "    #Remove Stop Words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(s)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    #print(filtered_sentence)\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "            ingredients.append(w)\n",
    "    s=' '.join(filtered_sentence)\n",
    "    \n",
    "    \n",
    "    #Remove low-content adjectives\n",
    "    \n",
    "    \n",
    "    #Porter Stemmer Algorithm\n",
    "    words = word_tokenize(s)\n",
    "    word_ps=[]\n",
    "    for w in words:\n",
    "        word_ps.append(ps.stem(w))\n",
    "    s=' '.join(word_ps)\n",
    "    ingredients.append(word_ps)\n",
    "    \n",
    "    l.append(s)\n",
    "json_df['ing_mod']=l\n",
    "print(json_df.head(10))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ingredientes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d0d348f8076f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mingredientes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ingredientes' is not defined"
     ]
    }
   ],
   "source": [
    "ingredientes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, 'other vegetables', 'liquor (appetizer)', 'sweet spreads', 'liquor', 'ice cream', 'syrup', 'tropical fruit', 'bathroom cleaner', 'potted plants', 'shopping bags', 'cling film/bags', 'turkey', 'beverages', 'salad dressing', 'hamburger meat', 'white wine', 'organic sausage', 'salt', 'yogurt', 'brandy', 'cake bar', 'chocolate marshmallow', 'butter', 'snack products', 'baby cosmetics', 'frozen fruits', 'curd', 'pickled vegetables', 'prosecco', 'ready soups', 'sliced cheese', 'honey', 'artif. sweetener', 'hair spray', 'candy', 'napkins', 'soups', 'flower (seeds)', 'dishes', 'pork', 'cooking chocolate', 'rum', 'ham', 'dessert', 'potato products', 'onions', 'berries', 'cat food', 'chocolate', 'kitchen towels', 'newspapers', 'whole milk', 'sparkling wine', 'light bulbs', 'grapes', 'pudding powder', 'nuts/prunes', 'packaged fruit/vegetables', 'Instant food products', 'frozen potato products', 'specialty bar', 'meat spreads', 'cereals', 'oil', 'sausage', 'photo/film', 'frankfurter', 'instant coffee', 'dog food', 'specialty cheese', 'brown bread', 'male cosmetics', 'meat', 'pet care', 'whisky', 'herbs', 'baking powder', 'detergent', 'hygiene articles', 'misc. beverages', 'house keeping products', 'chewing gum', 'spread cheese', 'skin care', 'tea', 'frozen meals', 'pastry', 'coffee', 'frozen chicken', 'liqueur', 'whipped/sour cream', 'cocoa drinks', 'waffles', 'semi-finished bread', 'root vegetables', 'fruit/vegetable juice', 'mayonnaise', 'candles', 'nut snack', 'sugar', 'pasta', 'liver loaf', 'flour', 'popcorn', 'dental care', 'canned fish', 'rubbing alcohol', 'specialty fat', 'curd cheese', 'chicken', 'specialty chocolate', 'domestic eggs', 'seasonal products', 'softener', 'margarine', 'canned beer', 'bottled beer', 'toilet cleaner', 'cookware', 'rolls/buns', 'pip fruit', 'long life bakery product', 'mustard', 'decalcifier', 'salty snack', 'zwieback', 'processed cheese', 'specialty vegetables', 'condensed milk', 'kitchen utensil', 'finished products', 'soap', 'soft cheese', 'hard cheese', 'canned fruit', 'canned vegetables', 'UHT-milk', 'abrasive cleaner', 'frozen dessert', 'organic products', 'butter milk', 'beef', 'bags', 'cream cheese', 'flower soil/fertilizer', 'preservation products', 'roll products', 'jam', 'frozen fish', 'cleaner', 'dish cleaner', 'soda', 'citrus fruit', 'make up remover', 'red/blush wine', 'frozen vegetables', 'bottled water', 'fish', 'sauces', 'baby food', 'spices', 'tidbits', 'rice', 'ketchup', 'cream', 'white bread', 'sound storage medium', 'vinegar', 'female sanitary products']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "testList = list(set(df.as_matrix().reshape((1,-1)).tolist()[0]))\n",
    "no_integers = [x for x in testList if not isinstance(x, int) ]\n",
    "print (no_integers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data argument can't be an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1b5ea1a93497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mapyori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Item(s)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-1b5ea1a93497>\u001b[0m in \u001b[0;36mapyori\u001b[0;34m(df, minimum_support, confidence)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'produto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'frequencia'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdf_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'produto'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nan'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'produto'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'None'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'frequencia'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data argument can't be an iterator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: data argument can't be an iterator"
     ]
    }
   ],
   "source": [
    "def apyori(df, minimum_support=0.1, confidence=0.22):\n",
    "    df_values = df.values.astype(str)\n",
    "    index, counts = np.unique(df_values,return_counts=True)\n",
    "    df_item = pd.DataFrame(zip(index, counts), columns = ['produto', 'frequencia'])\n",
    "    df_item.drop(df_item[(df_item['produto'] == 'nan' )|(df_item['produto'] == 'None' )].index, inplace=True)\n",
    "    df_item.sort_values(by='frequencia', ascending=False, inplace=True)\n",
    "    df_item.reset_index(drop=True, inplace=True)\n",
    "    df_item_frequent = df_item[df_item['frequencia']>= minimum_support*len(df)]\n",
    "    df_itemset_frequencia = pd.DataFrame(columns=['itemset', 'frequencia'])\n",
    "    for i in range(1, len(df_item_frequent)+1):\n",
    "        comb = list(combinations(df_item_frequent['produto'].values, i) )\n",
    "        for w in comb:\n",
    "            count = 0 \n",
    "            for instancia in df_values:\n",
    "                if all(elem in instancia  for elem in w):\n",
    "                    count = count +1\n",
    "            if count >= (minimum_support*len(df)/2):#tirar /2\n",
    "                df_itemset_frequencia = df_itemset_frequencia.append({'itemset':w, 'frequencia':count}, ignore_index=True)\n",
    "    df_itemset_frequencia.sort_values(by='frequencia', inplace=True, ascending=False)\n",
    "    confiabilidade = pd.DataFrame(columns=['regras', 'frequencia', 'confiabilidade'])\n",
    "    for w in df_itemset_frequencia['itemset'].values:\n",
    "        w_p = list(permutations(w,len(w)))\n",
    "        for j in w_p:\n",
    "            #print (len(j[0]))\n",
    "\n",
    "            p_uniao = []\n",
    "            for i in range(len(j)):\n",
    "\n",
    "                count = 0 \n",
    "                for instancia in df_values:\n",
    "                    if all(elem in instancia  for elem in j[i:]):\n",
    "                        count = count +1\n",
    "                p_uniao.append(count/len(df))\n",
    "\n",
    "            if len(j) != 1:\n",
    "                a = p_uniao[-2]/p_uniao[-1]\n",
    "\n",
    "                for i in range(len(p_uniao)-2):\n",
    "                    a = p_uniao[-i-3]/a\n",
    "                j = list(j)\n",
    "                j.reverse()\n",
    "                confiabilidade = confiabilidade.append({'regras':j, 'frequency':p_uniao[0], 'confidence':a}, ignore_index=True)\n",
    "            else:\n",
    "                confiabilidade = confiabilidade.append({'regras':j, 'frequency':p_uniao[0], 'confidence':p_uniao[0]}, ignore_index=True)\n",
    "    confiabilidade.sort_values(by='frequency', ascending=False)\n",
    "    return confiabilidade[confiabilidade['confidence']>=confidence]\n",
    "\n",
    "\n",
    "apyori(df.drop(columns='Item(s)'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
