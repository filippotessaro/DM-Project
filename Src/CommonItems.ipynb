{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/start/groceries.csv\n",
      "../Data/start/test.json\n",
      "../Data/start/train.json\n",
      "../Data/start/groceries - groceries.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.special import comb\n",
    "from itertools import combinations, permutations\n",
    "\n",
    "import os\n",
    "\n",
    "# List all datests files\n",
    "for dirname, _, filenames in os.walk('../Data/start'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique fields\n",
    "#print(json_df['cuisine'].unique())\n",
    "#print(json_df['ingredients'].unique())\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "def itemParser(row):\n",
    "    \"\"\"\n",
    "    Row items cleaner. \n",
    "  \n",
    "    This function is useful to normalize the items of the chart  \n",
    "    \n",
    "    Parameters: \n",
    "    row (list): Row of chart items\n",
    "  \n",
    "    Returns: \n",
    "    list: Normalized items \n",
    "\n",
    "    \"\"\"\n",
    "    word_ps = []\n",
    "    for s in row:\n",
    "        \n",
    "        #REGULAR EXPRESSIONS:\n",
    "\n",
    "        #Remove punctuations\n",
    "        s = re.sub(r'[^\\w\\s]', '', s)\n",
    "\n",
    "        #Remove Digits\n",
    "        s = re.sub(r\"(\\d)\", \"\", s)\n",
    "\n",
    "        #Remove content inside paranthesis\n",
    "        s = re.sub(r'\\([^)]*\\)', '', s)\n",
    "\n",
    "        #Remove Brand Name\n",
    "        s = re.sub(u'\\w*\\u2122', '', s)\n",
    "\n",
    "        #Convert to lowercase\n",
    "        s = s.lower()\n",
    "\n",
    "        #print(ps.stem(s))\n",
    "        #Remove Stop Words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = word_tokenize(s)\n",
    "        \n",
    "        filtered_sentence = [ps.stem(w) for w in word_tokens if not w in stop_words]\n",
    "        #print(filtered_sentence)\n",
    "        s = ' '.join(filtered_sentence)\n",
    "        #print(\"join:\" + s)\n",
    "        word_ps.append(s)\n",
    "    return word_ps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groceries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = []\n",
    "with open('../Data/start/groceries.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        chart.append(itemParser(line[:-1].split(',')))\n",
    "        #chart.append(line[:-1].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n"
     ]
    }
   ],
   "source": [
    "chart[0:10]\n",
    "groceries_items = set()\n",
    "for row in chart:\n",
    "    groceries_items.update(row)\n",
    "\n",
    "print(len(groceries_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = pd.read_json ('../Data/start/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6714\n"
     ]
    }
   ],
   "source": [
    "list_ingredients = set()\n",
    "for item in json_df.ingredients:\n",
    "    #item = [lower(i) for i in item]\n",
    "    list_ingredients.update(item)\n",
    "    \n",
    "print(len(list_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>new_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10259</td>\n",
       "      <td>greek</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "      <td>[romain lettuc, black oliv, grape tomato, garl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25693</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomato, gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20130</td>\n",
       "      <td>filipino</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "      <td>[egg, pepper, salt, mayonais, cook oil, green ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22213</td>\n",
       "      <td>indian</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "      <td>[water, veget oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13162</td>\n",
       "      <td>indian</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "      <td>[black pepper, shallot, cornflour, cayenn pepp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      cuisine                                        ingredients  \\\n",
       "0  10259        greek  [romaine lettuce, black olives, grape tomatoes...   \n",
       "1  25693  southern_us  [plain flour, ground pepper, salt, tomatoes, g...   \n",
       "2  20130     filipino  [eggs, pepper, salt, mayonaise, cooking oil, g...   \n",
       "3  22213       indian                [water, vegetable oil, wheat, salt]   \n",
       "4  13162       indian  [black pepper, shallots, cornflour, cayenne pe...   \n",
       "\n",
       "                                     new_ingredients  \n",
       "0  [romain lettuc, black oliv, grape tomato, garl...  \n",
       "1  [plain flour, ground pepper, salt, tomato, gro...  \n",
       "2  [egg, pepper, salt, mayonais, cook oil, green ...  \n",
       "3                    [water, veget oil, wheat, salt]  \n",
       "4  [black pepper, shallot, cornflour, cayenn pepp...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df['new_ingredients'] = json_df.apply(lambda x: itemParser(x.ingredients), axis=1)\n",
    "json_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export clean dataframe to csv\n",
    "json_df.to_csv('./data/train-clean.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Similar Items\n",
    "We would test the similarity of the two datasets computing their intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groceries_items\n",
    "ingredients_items = []\n",
    "for row in json_df['new_ingredients']:\n",
    "    for item in row:\n",
    "        ingredients_items.append(item)\n",
    "        \n",
    "#print(\"Before: \" + str(len(ingredients_items)))\n",
    "#ingredients_items[0:10]\n",
    "ingredients_items = set(ingredients_items)\n",
    "#print(\"After: \" + str(len(ingredients_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "common = groceries_items.intersection(ingredients_items)\n",
    "uncommon = groceries_items.union(ingredients_items) - common\n",
    "\n",
    "#Before 59\n",
    "print(len(common))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
