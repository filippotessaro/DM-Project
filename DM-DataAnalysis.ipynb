{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/groceries_aligned.csv\n",
      "./data/test.json\n",
      "./data/train-clean.csv\n",
      "./data/groceries.csv\n",
      "./data/train.json\n",
      "./data/groceries - groceries.csv\n",
      "./data/.ipynb_checkpoints/groceries-checkpoint.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.special import comb\n",
    "from itertools import combinations, permutations\n",
    "\n",
    "import os\n",
    "\n",
    "# List all datests files\n",
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique fields\n",
    "#print(json_df['cuisine'].unique())\n",
    "#print(json_df['ingredients'].unique())\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "def itemParser(row):\n",
    "    \"\"\"\n",
    "    Row items cleaner. \n",
    "  \n",
    "    This function is useful to normalize the items of the chart  \n",
    "    \n",
    "    Parameters: \n",
    "    row (list): Row of chart items\n",
    "  \n",
    "    Returns: \n",
    "    list: Normalized items \n",
    "\n",
    "    \"\"\"\n",
    "    word_ps = []\n",
    "    for s in row:\n",
    "        \n",
    "        #REGULAR EXPRESSIONS:\n",
    "\n",
    "        #Remove punctuations\n",
    "        s = re.sub(r'[^\\w\\s]', '', s)\n",
    "\n",
    "        #Remove Digits\n",
    "        s = re.sub(r\"(\\d)\", \"\", s)\n",
    "\n",
    "        #Remove content inside paranthesis\n",
    "        s = re.sub(r'\\([^)]*\\)', '', s)\n",
    "\n",
    "        #Remove Brand Name\n",
    "        s = re.sub(u'\\w*\\u2122', '', s)\n",
    "\n",
    "        #Convert to lowercase\n",
    "        s = s.lower()\n",
    "\n",
    "        #print(ps.stem(s))\n",
    "        #Remove Stop Words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = word_tokenize(s)\n",
    "        \n",
    "        filtered_sentence = [ps.stem(w) for w in word_tokens if not w in stop_words]\n",
    "        #print(filtered_sentence)\n",
    "        s = ' '.join(filtered_sentence)\n",
    "        #print(\"join:\" + s)\n",
    "        word_ps.append(s)\n",
    "    return word_ps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(chart, minimum_support=0.1, confidence=0.22):\n",
    "    \n",
    "    #groceries_items = set()\n",
    "    \n",
    "    mapping = dict()\n",
    "    singleton_count = dict()\n",
    "    uuid = 0\n",
    "    \n",
    "    for row in chart:\n",
    "        for item in row:\n",
    "            if item not in mapping.keys():\n",
    "                mapping[item] = uuid\n",
    "                singleton_count[item] = 0\n",
    "                uuid += 1\n",
    "            singleton_count[item] += 1\n",
    "    #Take mapping \n",
    "    singleton_count = {mapping[k]: v for (k,v) in singleton_count.items() if v > minimum_support}\n",
    "    print(singleton_count)\n",
    "    #a1_sorted_keys = sorted(singleton_count, key=singleton_count.get, reverse=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groceries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = []\n",
    "with open('./data/groceries.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        chart.append(itemParser(line[:-1].split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 814, 1: 174, 2: 576, 3: 18, 4: 1032, 5: 1372, 6: 571, 7: 2513, 8: 744, 9: 390, 10: 42, 11: 1903, 12: 101, 13: 368, 14: 545, 15: 75, 16: 35, 17: 1809, 18: 329, 19: 792, 20: 78, 21: 170, 22: 56, 23: 414, 24: 1087, 25: 488, 26: 524, 27: 171, 28: 173, 29: 516, 30: 580, 31: 1715, 32: 422, 33: 333, 34: 711, 35: 785, 36: 128, 37: 269, 38: 275, 39: 875, 40: 163, 41: 189, 42: 1072, 43: 106, 44: 89, 45: 372, 46: 378, 47: 294, 48: 27, 49: 764, 50: 924, 51: 638, 52: 969, 53: 256, 54: 327, 55: 51, 56: 324, 57: 515, 58: 567, 59: 327, 60: 705, 61: 32, 62: 220, 63: 365, 64: 68, 65: 624, 66: 110, 67: 279, 68: 241, 69: 229, 70: 256, 71: 80, 72: 174, 73: 176, 74: 276, 75: 207, 76: 89, 77: 246, 78: 473, 79: 148, 80: 140, 81: 50, 82: 189, 83: 83, 84: 36, 85: 299, 86: 88, 87: 102, 88: 55, 89: 106, 90: 279, 91: 106, 92: 305, 93: 160, 94: 187, 95: 41, 96: 91, 97: 241, 98: 148, 99: 54, 100: 112, 101: 29, 102: 45, 103: 32, 104: 79, 105: 168, 106: 15, 107: 57, 108: 71, 109: 130, 110: 30, 111: 19, 112: 84, 113: 64, 114: 22, 115: 84, 116: 20, 117: 115, 118: 8, 119: 50, 120: 60, 121: 103, 122: 27, 123: 254, 124: 38, 125: 118, 126: 82, 127: 35, 128: 28, 129: 109, 130: 93, 131: 67, 132: 44, 133: 8, 134: 54, 135: 64, 136: 26, 137: 11, 138: 73, 139: 101, 140: 90, 141: 10, 142: 32, 143: 50, 144: 6, 145: 16, 146: 31, 147: 59, 148: 6, 149: 41, 150: 42, 151: 53, 152: 15, 153: 33, 154: 9, 155: 22, 156: 13, 157: 7, 158: 17, 160: 23, 161: 23, 162: 8, 163: 12, 164: 4, 165: 25, 167: 4}\n"
     ]
    }
   ],
   "source": [
    "#print(chart[:10])\n",
    "apriori(chart, minimum_support=3, confidence=0.22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n"
     ]
    }
   ],
   "source": [
    "chart[0:10]\n",
    "groceries_items = set()\n",
    "for row in chart:\n",
    "    groceries_items.update(row)\n",
    "\n",
    "print(len(groceries_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = pd.read_json ('./data/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6714\n"
     ]
    }
   ],
   "source": [
    "list_ingredients = set()\n",
    "for item in json_df.ingredients:\n",
    "    #item = [lower(i) for i in item]\n",
    "    list_ingredients.update(item)\n",
    "    \n",
    "print(len(list_ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>new_ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "      <td>[romain lettuc, black oliv, grape tomato, garl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomato, gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "      <td>[egg, pepper, salt, mayonais, cook oil, green ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "      <td>[water, veget oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "      <td>[black pepper, shallot, cornflour, cayenn pepp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients  \\\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...   \n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...   \n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...   \n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]   \n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe...   \n",
       "\n",
       "                                     new_ingredients  \n",
       "0  [romain lettuc, black oliv, grape tomato, garl...  \n",
       "1  [plain flour, ground pepper, salt, tomato, gro...  \n",
       "2  [egg, pepper, salt, mayonais, cook oil, green ...  \n",
       "3                    [water, veget oil, wheat, salt]  \n",
       "4  [black pepper, shallot, cornflour, cayenn pepp...  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df['new_ingredients'] = json_df.apply(lambda x: itemParser(x.ingredients), axis=1)\n",
    "json_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export clean dataframe to csv\n",
    "json_df.to_csv('./data/train-clean.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Similar Items\n",
    "We would test the similarity of the two datasets computing their intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groceries_items\n",
    "ingredients_items = []\n",
    "for row in json_df['new_ingredients']:\n",
    "    for item in row:\n",
    "        ingredients_items.append(item)\n",
    "        \n",
    "#print(\"Before: \" + str(len(ingredients_items)))\n",
    "#ingredients_items[0:10]\n",
    "ingredients_items = set(ingredients_items)\n",
    "#print(\"After: \" + str(len(ingredients_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'can beer', 'light bulb', 'toilet cleaner', 'liquor appet', 'tidbit', 'curd chees', 'preserv product', 'kitchen utensil', 'soften', 'dog food', 'liver loaf', 'redblush wine', 'season product', 'chocol marshmallow', 'cocoa drink', 'abras cleaner', 'cookwar', 'long life bakeri product', 'dish', 'dental care', 'rub alcohol', 'hair spray', 'misc beverag', 'dish cleaner', 'zwieback', 'decalcifi', 'whippedsour cream', 'can fruit', 'hamburg meat', 'cook chocol', 'specialti chees', 'male cosmet', 'photofilm', 'butter milk', 'finish product', 'readi soup', 'whiski', 'make remov', 'napkin', 'cleaner', 'bathroom cleaner', 'hous keep product', 'spread chees', 'fruitveget juic', 'brown bread', 'specialti chocol', 'can veget', 'uhtmilk', 'nutsprun', 'cling filmbag', 'sweet spread', 'nut snack', 'can fish', 'bag', 'specialti fat', 'salti snack', 'deterg', 'beverag', 'frozen fish', 'newspap', 'pip fruit', 'mayonnais', 'potato product', 'sound storag medium', 'bottl water', 'artif sweeten', 'femal sanitari product', 'frozen potato product', 'shop bag', 'babi cosmet', 'rollsbun', 'frozen meal', 'flower soilfertil', 'flower seed', 'domest egg', 'specialti bar', 'packag fruitveget', 'specialti veget', 'frozen chicken', 'bottl beer', 'instant food product', 'pet care', 'cat food', 'roll product', 'snack product', 'semifinish bread', 'organ sausag', 'babi food', 'soap', 'meat spread', 'chew gum', 'dessert', 'slice chees', 'cake bar', 'candl', 'pot plant', 'hygien articl', 'yogurt', 'frozen dessert', 'soft chees', 'organ product', 'kitchen towel', 'skin care'}\n"
     ]
    }
   ],
   "source": [
    "common = groceries_items.intersection(ingredients_items)\n",
    "uncommon = groceries_items.union(ingredients_items) - common\n",
    "\n",
    "print(ingredients_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['romaine lettuce,black olives,grape tomatoes,garlic,pepper,purple onion,seasoning,garbanzo beans,feta cheese crumbles', 'plain flour,ground pepper,salt,tomatoes,ground black pepper,thyme,eggs,green tomatoes,yellow corn meal,milk,vegetable oil', 'eggs,pepper,salt,mayonaise,cooking oil,green chilies,grilled chicken breasts,garlic powder,yellow onion,soy sauce,butter,chicken livers']\n",
      "       cuisine     id                                        ingredients  \\\n",
      "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...   \n",
      "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...   \n",
      "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...   \n",
      "3       indian  22213                [water, vegetable oil, wheat, salt]   \n",
      "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe...   \n",
      "5     jamaican   6602  [plain flour, sugar, butter, eggs, fresh ginge...   \n",
      "6      spanish  42779  [olive oil, salt, medium shrimp, pepper, garli...   \n",
      "7      italian   3735  [sugar, pistachio nuts, white almond bark, flo...   \n",
      "8      mexican  16903  [olive oil, purple onion, fresh pineapple, por...   \n",
      "9      italian  12734  [chopped tomatoes, fresh basil, garlic, extra-...   \n",
      "\n",
      "                                                 ing  \\\n",
      "0  romaine lettuce,black olives,grape tomatoes,ga...   \n",
      "1  plain flour,ground pepper,salt,tomatoes,ground...   \n",
      "2  eggs,pepper,salt,mayonaise,cooking oil,green c...   \n",
      "3                     water,vegetable oil,wheat,salt   \n",
      "4  black pepper,shallots,cornflour,cayenne pepper...   \n",
      "5  plain flour,sugar,butter,eggs,fresh ginger roo...   \n",
      "6  olive oil,salt,medium shrimp,pepper,garlic,cho...   \n",
      "7  sugar,pistachio nuts,white almond bark,flour,v...   \n",
      "8  olive oil,purple onion,fresh pineapple,pork,po...   \n",
      "9  chopped tomatoes,fresh basil,garlic,extra-virg...   \n",
      "\n",
      "                                             ing_mod  \n",
      "0  romain lettuc , black oliv , grape tomato , ga...  \n",
      "1  plain flour , ground pepper , salt , tomato , ...  \n",
      "2  egg , pepper , salt , mayonais , cook oil , gr...  \n",
      "3                   water , veget oil , wheat , salt  \n",
      "4  black pepper , shallot , cornflour , cayenn pe...  \n",
      "5  plain flour , sugar , butter , egg , fresh gin...  \n",
      "6  oliv oil , salt , medium shrimp , pepper , gar...  \n",
      "7  sugar , pistachio nut , white almond bark , fl...  \n",
      "8  oliv oil , purpl onion , fresh pineappl , pork...  \n",
      "9  chop tomato , fresh basil , garlic , extra-vir...  \n"
     ]
    }
   ],
   "source": [
    "'''# Unique fields\n",
    "#print(json_df['cuisine'].unique())\n",
    "#print(json_df['ingredients'].unique())\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('english')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "new = []\n",
    "for s in json_df['ingredients']:\n",
    "    #print(str(s) + \"\\n\")\n",
    "    s = ','.join(s)\n",
    "    new.append(s)\n",
    "    \n",
    "print(new[0:3])\n",
    "\n",
    "json_df['ing'] = new\n",
    "\n",
    "ingredients = list()\n",
    "\n",
    "l=[]\n",
    "for s in json_df['ing']:\n",
    "    #REGULAR EXPRESSIONS:\n",
    "    \n",
    "    #Remove punctuations\n",
    "    #s=re.sub(r'[^\\w\\s]','',s)\n",
    "    \n",
    "    #Remove Digits\n",
    "    s=re.sub(r\"(\\d)\", \"\", s)\n",
    "    \n",
    "    #Remove content inside paranthesis\n",
    "    s=re.sub(r'\\([^)]*\\)', '', s)\n",
    "    \n",
    "    #Remove Brand Name\n",
    "    s=re.sub(u'\\w*\\u2122', '', s)\n",
    "    \n",
    "    #Convert to lowercase\n",
    "    s=s.lower()\n",
    "    \n",
    "    #Remove Stop Words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(s)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    #print(filtered_sentence)\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "            ingredients.append(w)\n",
    "    s=' '.join(filtered_sentence)\n",
    "    \n",
    "    \n",
    "    #Remove low-content adjectives\n",
    "    \n",
    "    \n",
    "    #Porter Stemmer Algorithm\n",
    "    words = word_tokenize(s)\n",
    "    word_ps=[]\n",
    "    for w in words:\n",
    "        word_ps.append(ps.stem(w))\n",
    "    s=' '.join(word_ps)\n",
    "    ingredients.append(word_ps)\n",
    "    \n",
    "    l.append(s)\n",
    "json_df['ing_mod']=l\n",
    "print(json_df.head(10))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ingredientes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d0d348f8076f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mingredientes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ingredientes' is not defined"
     ]
    }
   ],
   "source": [
    "ingredientes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, 'other vegetables', 'liquor (appetizer)', 'sweet spreads', 'liquor', 'ice cream', 'syrup', 'tropical fruit', 'bathroom cleaner', 'potted plants', 'shopping bags', 'cling film/bags', 'turkey', 'beverages', 'salad dressing', 'hamburger meat', 'white wine', 'organic sausage', 'salt', 'yogurt', 'brandy', 'cake bar', 'chocolate marshmallow', 'butter', 'snack products', 'baby cosmetics', 'frozen fruits', 'curd', 'pickled vegetables', 'prosecco', 'ready soups', 'sliced cheese', 'honey', 'artif. sweetener', 'hair spray', 'candy', 'napkins', 'soups', 'flower (seeds)', 'dishes', 'pork', 'cooking chocolate', 'rum', 'ham', 'dessert', 'potato products', 'onions', 'berries', 'cat food', 'chocolate', 'kitchen towels', 'newspapers', 'whole milk', 'sparkling wine', 'light bulbs', 'grapes', 'pudding powder', 'nuts/prunes', 'packaged fruit/vegetables', 'Instant food products', 'frozen potato products', 'specialty bar', 'meat spreads', 'cereals', 'oil', 'sausage', 'photo/film', 'frankfurter', 'instant coffee', 'dog food', 'specialty cheese', 'brown bread', 'male cosmetics', 'meat', 'pet care', 'whisky', 'herbs', 'baking powder', 'detergent', 'hygiene articles', 'misc. beverages', 'house keeping products', 'chewing gum', 'spread cheese', 'skin care', 'tea', 'frozen meals', 'pastry', 'coffee', 'frozen chicken', 'liqueur', 'whipped/sour cream', 'cocoa drinks', 'waffles', 'semi-finished bread', 'root vegetables', 'fruit/vegetable juice', 'mayonnaise', 'candles', 'nut snack', 'sugar', 'pasta', 'liver loaf', 'flour', 'popcorn', 'dental care', 'canned fish', 'rubbing alcohol', 'specialty fat', 'curd cheese', 'chicken', 'specialty chocolate', 'domestic eggs', 'seasonal products', 'softener', 'margarine', 'canned beer', 'bottled beer', 'toilet cleaner', 'cookware', 'rolls/buns', 'pip fruit', 'long life bakery product', 'mustard', 'decalcifier', 'salty snack', 'zwieback', 'processed cheese', 'specialty vegetables', 'condensed milk', 'kitchen utensil', 'finished products', 'soap', 'soft cheese', 'hard cheese', 'canned fruit', 'canned vegetables', 'UHT-milk', 'abrasive cleaner', 'frozen dessert', 'organic products', 'butter milk', 'beef', 'bags', 'cream cheese', 'flower soil/fertilizer', 'preservation products', 'roll products', 'jam', 'frozen fish', 'cleaner', 'dish cleaner', 'soda', 'citrus fruit', 'make up remover', 'red/blush wine', 'frozen vegetables', 'bottled water', 'fish', 'sauces', 'baby food', 'spices', 'tidbits', 'rice', 'ketchup', 'cream', 'white bread', 'sound storage medium', 'vinegar', 'female sanitary products']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "testList = list(set(df.as_matrix().reshape((1,-1)).tolist()[0]))\n",
    "no_integers = [x for x in testList if not isinstance(x, int) ]\n",
    "print (no_integers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data argument can't be an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1b5ea1a93497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mapyori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Item(s)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-1b5ea1a93497>\u001b[0m in \u001b[0;36mapyori\u001b[0;34m(df, minimum_support, confidence)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'produto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'frequencia'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdf_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'produto'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nan'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'produto'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'None'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'frequencia'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data argument can't be an iterator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: data argument can't be an iterator"
     ]
    }
   ],
   "source": [
    "def apyori(df, minimum_support=0.1, confidence=0.22):\n",
    "    df_values = df.values.astype(str)\n",
    "    index, counts = np.unique(df_values,return_counts=True)\n",
    "    df_item = pd.DataFrame(zip(index, counts), columns = ['produto', 'frequencia'])\n",
    "    df_item.drop(df_item[(df_item['produto'] == 'nan' )|(df_item['produto'] == 'None' )].index, inplace=True)\n",
    "    df_item.sort_values(by='frequencia', ascending=False, inplace=True)\n",
    "    df_item.reset_index(drop=True, inplace=True)\n",
    "    df_item_frequent = df_item[df_item['frequencia']>= minimum_support*len(df)]\n",
    "    df_itemset_frequencia = pd.DataFrame(columns=['itemset', 'frequencia'])\n",
    "    for i in range(1, len(df_item_frequent)+1):\n",
    "        comb = list(combinations(df_item_frequent['produto'].values, i) )\n",
    "        for w in comb:\n",
    "            count = 0 \n",
    "            for instancia in df_values:\n",
    "                if all(elem in instancia  for elem in w):\n",
    "                    count = count +1\n",
    "            if count >= (minimum_support*len(df)/2):#tirar /2\n",
    "                df_itemset_frequencia = df_itemset_frequencia.append({'itemset':w, 'frequencia':count}, ignore_index=True)\n",
    "    df_itemset_frequencia.sort_values(by='frequencia', inplace=True, ascending=False)\n",
    "    confiabilidade = pd.DataFrame(columns=['regras', 'frequencia', 'confiabilidade'])\n",
    "    for w in df_itemset_frequencia['itemset'].values:\n",
    "        w_p = list(permutations(w,len(w)))\n",
    "        for j in w_p:\n",
    "            #print (len(j[0]))\n",
    "\n",
    "            p_uniao = []\n",
    "            for i in range(len(j)):\n",
    "\n",
    "                count = 0 \n",
    "                for instancia in df_values:\n",
    "                    if all(elem in instancia  for elem in j[i:]):\n",
    "                        count = count +1\n",
    "                p_uniao.append(count/len(df))\n",
    "\n",
    "            if len(j) != 1:\n",
    "                a = p_uniao[-2]/p_uniao[-1]\n",
    "\n",
    "                for i in range(len(p_uniao)-2):\n",
    "                    a = p_uniao[-i-3]/a\n",
    "                j = list(j)\n",
    "                j.reverse()\n",
    "                confiabilidade = confiabilidade.append({'regras':j, 'frequency':p_uniao[0], 'confidence':a}, ignore_index=True)\n",
    "            else:\n",
    "                confiabilidade = confiabilidade.append({'regras':j, 'frequency':p_uniao[0], 'confidence':p_uniao[0]}, ignore_index=True)\n",
    "    confiabilidade.sort_values(by='frequency', ascending=False)\n",
    "    return confiabilidade[confiabilidade['confidence']>=confidence]\n",
    "\n",
    "\n",
    "apyori(df.drop(columns='Item(s)'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe copy\n",
    "tmp = json_df[0:5].copy()\n",
    "print(tmp.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
